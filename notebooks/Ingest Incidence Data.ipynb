{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest sample incidence data from Caltrans\n",
    "\n",
    "Here we:\n",
    "1. Ingest txt files from the Caltrans clearinhouse containing incident information into pandas form\n",
    "2. Process the data by removing other districts and cleaning up some minor issues\n",
    "3. Load the data to a table in OmniSci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T14:03:52.555325Z",
     "start_time": "2019-10-25T14:03:51.729550Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/mapdadmin/abraham/caltrans-data-exploration/')\n",
    "config_path = '/home/mapdadmin/abraham/ini_files/config.ini'\n",
    "from omnisci_connector.omni_connect import OmnisciConnect\n",
    "import data_processing.process_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T14:03:52.561977Z",
     "start_time": "2019-10-25T14:03:52.558170Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pymapd\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T14:03:52.831322Z",
     "start_time": "2019-10-25T14:03:52.564137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read configuration file /home/mapdadmin/abraham/ini_files/config.ini\n",
      "Configuration file read.\n",
      "connect to omnisci\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Connection(omnisci://abraham:***@http://localhost:6273/abraham?protocol=http)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"read configuration file %s\" %config_path)\n",
    "config = ConfigParser()\n",
    "config.read(config_path)\n",
    "print(\"Configuration file read.\")\n",
    "\n",
    "print(\"connect to omnisci\")\n",
    "OmnisciHandle = OmnisciConnect(config_path)\n",
    "OmnisciHandle.start_connection()\n",
    "OmnisciHandle.con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to OmniSci by using PyMaPD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the raw data:\n",
    "_Provided from Caltrans_\n",
    "\n",
    "* **Incident ID**\tAn integer value that uniquely identifies this incident within PeMS.\t \n",
    "* **Timestamp**\tDate and time of the incident with a format of MM/DD/YYYY HH24:MI:SS. For example 9/3/2013 13:58, indicating 9/3/2013 1:58 PM.\t \n",
    "* **Description**\tA textual description of the incident.\t \n",
    "* **Location**\tA textual description of the location.\t \n",
    "* **Area**\tA textual description of the Area. For example, East Sac.\t \n",
    "* **Latitude**\tLatitude\t \n",
    "* **Longitude**\tLongitude\t \n",
    "* **District**\tthe District number\t \n",
    "* **Freeway Number**\tFreeway Number\t \n",
    "* **Freeway Direction**\tA string indicating the freeway direction.\t \n",
    "* **State Postmile**\tState Postmile\t \n",
    "* **Absolute Postmile**\tAbsolute Postmile\t \n",
    "* **Duration** In minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T14:03:53.923150Z",
     "start_time": "2019-10-25T14:03:53.917349Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(filepaths, usecols, names):\n",
    "    l_df = []\n",
    "    for f in filepaths:\n",
    "        print(\"Processing file: \", f)\n",
    "        temp = pd.read_csv(f,header=None,usecols=usecols,names=names)\n",
    "        l_df.append(temp)\n",
    "\n",
    "    return pd.concat(l_df, ignore_index=True)\n",
    "\n",
    "def get_filepaths(path):\n",
    "    return [os.path.join(path, f) for f in os.listdir(path) if (os.path.isfile(os.path.join(path, f))) and (str.endswith(f,'txt'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T14:03:54.419205Z",
     "start_time": "2019-10-25T14:03:54.416112Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all filepaths\n",
    "path = '../incident_data/'\n",
    "filepaths = get_filepaths(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T14:03:55.302092Z",
     "start_time": "2019-10-25T14:03:55.004244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file:  ../incident_data/all_text_chp_incidents_month_2019_03.txt\n",
      "Processing file:  ../incident_data/all_text_chp_incidents_month_2019_02.txt\n",
      "Processing file:  ../incident_data/all_text_chp_incidents_month_2019_01.txt\n"
     ]
    }
   ],
   "source": [
    "usecols = [0,3,4,5,6,9,10,11,14,15,16,17,19]\n",
    "names = ['id', 'timestamp_', 'description','location','area','latitude','longitude', 'district','freeway','direction','postmile_state','postmile_abs','duration']\n",
    "\n",
    "df = read_data(filepaths, usecols, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the data\n",
    "\n",
    "* Change timestamp to datetime format\n",
    "* keep only SF district\n",
    "* clean up durations. Change negative and NULL durations to 0\n",
    "* Remove Freeway Service Patrol (FSP) from the area\n",
    "* separate the CHP (California Highway Patrol) Codes (http://cad.chp.ca.gov/htm.net/glossary.htm) and the actual description of the event\n",
    "* Change datatypes to the correct type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T14:03:56.912049Z",
     "start_time": "2019-10-25T14:03:56.337889Z"
    }
   },
   "outputs": [],
   "source": [
    "# clean up some data\n",
    "\n",
    "# change timestamp to datetime format\n",
    "df['timestamp_'] = pd.to_datetime(df['timestamp_'], infer_datetime_format=True)\n",
    "\n",
    "# change drop all other districts that aren't needed ( only 04 is needed). Change datatype to int\n",
    "df = df.dropna(subset=['district'])\n",
    "df['district'] = df['district'].astype(int)\n",
    "df = (df.loc[df['district'] == 4]\n",
    "      .drop('district',axis=1)\n",
    "      .set_index('id')\n",
    "     )\n",
    "\n",
    "# change negative and NaN durations to 0. Why are they negative? idk!\n",
    "df.loc[df['duration']<0,'duration'] = 0\n",
    "df.loc[df['duration'].isna(),'duration'] = 0.0\n",
    "\n",
    "# Clean up the area. Remove FSP and unnecessary whitespaces\n",
    "df['area'] = df['area'].str.replace(\"FSP\", \"\").str.strip()\n",
    "\n",
    "# Separate the chp code and the incident description\n",
    "df['chp_code'] = df['description'].str.split('-',expand=True)[0]\n",
    "df['incident_description'] = df['description'].str.split('-',expand=True)[1]\n",
    "df.loc[df.incident_description.isna(), 'incident_description'] = df.chp_code.loc[df.incident_description.isna()]\n",
    "df.incident_description = df.incident_description.str.lower()\n",
    "\n",
    "# Change to correct datatypes\n",
    "df['duration'] = df.duration.astype(np.int32)\n",
    "df['freeway'] = df.freeway.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Feature Engineering\n",
    "\n",
    "1. Add a is_ramp column that checks whether the incident was reported at an offramp or onramp\n",
    "2. create a severity measure of the incidents (subjective)\n",
    "3. add a day_of_week and hour of day column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring of severity of incident:\n",
    "* 0 - CHP/Caltrans\n",
    "* 1 - hazard\n",
    "* 2 - Collision/Severe incident\n",
    "\n",
    "\\*Caltrans uses the term \"SigAlert\" and defines it as any traffic incident that will tie up two or more lanes of a freeway for two or more hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T14:54:39.636909Z",
     "start_time": "2019-10-25T14:54:39.583840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a is_ramp column:\n",
    "df['is_ramp'] = df.location.str.contains('Ofr|Onr',na=False)\n",
    "\n",
    "# Add an is_intersection column:\n",
    "df['is_intersection'] = df['location'].str.count('/')==1\n",
    "# add an hour_of_day column\n",
    "df['hour_of_day'] = df.timestamp_.dt.hour.astype(np.int16)\n",
    "\n",
    "# add a day_of_week column:\n",
    "df['day_of_week'] = df.timestamp_.dt.dayofweek.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T14:54:41.668387Z",
     "start_time": "2019-10-25T14:54:41.651407Z"
    }
   },
   "outputs": [],
   "source": [
    "severity = {\n",
    "    'trfc collision': 2,\n",
    "    'assist with construction': 1,\n",
    "    'wrong way driver': 1,\n",
    "    'traffic hazard': 1,\n",
    "    'report of fire': 1,\n",
    "    'animal hazard': 1,\n",
    "     'hit and run no injuries': 2,\n",
    "     'defective traffic signals': 1,\n",
    "     'car fire': 2,\n",
    "     'live or dead animal': 1,\n",
    "     'traffic break': 1,\n",
    "     'request caltrans notify': 0,\n",
    "     'provide traffic control': 0,\n",
    "     'assist ct with maintenance': 0,\n",
    "     'spilled material inc': 1,\n",
    "     'hit and run w/injuries': 2,\n",
    "     'object flying from veh': 1,\n",
    "     'mud/dirt/rock': 1,\n",
    "     'req chp traffic control': 0,\n",
    "     'foggy conditions': 1,\n",
    "     'wind advisory': 1,\n",
    "     'traffic advisory': 1,\n",
    "     'spinout': 2,\n",
    "     'closure of a road': 1,\n",
    "     'hazardous materials inc': 1,\n",
    "     'sig alert': 2,\n",
    "     'roadway flooding': 1,\n",
    "     'road/weather conditions': 1,\n",
    "     'fsp req traffic break': 0,\n",
    "     'aircraft emergency': 1,\n",
    "     'fatality': 2,\n",
    "     'jumper': 1,\n",
    "}\n",
    "\n",
    "df['severity'] = df.incident_description.apply(lambda x: severity[x]).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T14:54:49.930308Z",
     "start_time": "2019-10-25T14:54:49.916303Z"
    }
   },
   "outputs": [],
   "source": [
    "# simplify the classes to CHP, hazard, and severe\n",
    "simplify = True\n",
    "\n",
    "if simplify:\n",
    "    simplified = {0:0, 1:0, 2:1}\n",
    "    df['severity_simple'] = df['severity'].apply(lambda x: simplified[x]).astype(np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the processed incident data to OmniSci or to txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T14:55:13.578068Z",
     "start_time": "2019-10-25T14:55:12.930688Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "save = True\n",
    "\n",
    "if save:\n",
    "    omnisci = True\n",
    "    table_name = 'incidents_jan19_apr19_191025'\n",
    "\n",
    "    if omnisci:\n",
    "        \n",
    "        OmnisciHandle.con.load_table(table_name, df.reset_index())\n",
    "    else:\n",
    "        df.to_csv('../incident_data/' + table_name + '.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining different tables\n",
    "\n",
    "- read in traffic meta table and find closest traffic station\n",
    "- read in table with weather and traffic from OmniSci\n",
    "- join with incident data (timestamp and lat,long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join traffic meta table with closest incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:23:17.288496Z",
     "start_time": "2019-10-24T23:23:17.269125Z"
    }
   },
   "outputs": [],
   "source": [
    "def queries(table, cols, date_range):\n",
    "    queries = []\n",
    "    for x in range(len(date_range)-1):\n",
    "        condition = \"WHERE timestamp_ >= '\" + date_range[x] + \" 00:00'\\\n",
    " AND timestamp_ <  '\" + date_range[x+1] + \" 00:00'\"\n",
    "        query = \"select \" + cols + \" from \" + table + \" \" + condition\n",
    "        queries.append(query)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:23:17.308709Z",
     "start_time": "2019-10-24T23:23:17.290507Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_large_table(handle, queries):\n",
    "    l_df = []\n",
    "    for q in queries:\n",
    "        print(q)\n",
    "        temp = handle.con.select_ipc(q)\n",
    "        l_df.append(temp)\n",
    "    \n",
    "    return pd.concat(l_df, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:23:24.906384Z",
     "start_time": "2019-10-24T23:23:17.310790Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in traffic meta table\n",
    "traffic_cols = 'station, direction, latitude, longitude'\n",
    "traffic_meta_table = 'caltrans_traffic_janfeb_encoded_strkey'\n",
    "\n",
    "date_list = ['2019-01-01', '2019-01-15', '2019-02-10','2019-03-08']\n",
    "traffic_q = queries(traffic_meta_table, traffic_cols, date_list)\n",
    "\n",
    "df_traffic_meta = read_large_table(OmnisciHandle, traffic_q)\n",
    "\n",
    "print(\"dropping duplicates\")\n",
    "df_traffic_meta = df_traffic_meta.drop_duplicates(subset=['station','direction'])\n",
    "df_traffic_meta = df_traffic_meta.rename(columns={'station':'id'})\n",
    "\n",
    "print(df_traffic_meta.shape)\n",
    "df_traffic_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:23:24.935116Z",
     "start_time": "2019-10-24T23:23:24.908867Z"
    }
   },
   "outputs": [],
   "source": [
    "df_locations = df.groupby('location')['direction','latitude','longitude'].first().reset_index()\n",
    "df_locations['direction'] = df_locations['direction'].astype('category')\n",
    "df_locations = df_locations.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:24:05.929339Z",
     "start_time": "2019-10-24T23:23:24.937240Z"
    }
   },
   "outputs": [],
   "source": [
    "# This may take a while, that's why it was needed to parallelize code\n",
    "dict_proxy = utils.longlat_distance_parallel(df_locations, df_traffic_meta, 'id',num_processes=20)\n",
    "\n",
    "location_station =  dict_proxy._getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:24:05.940416Z",
     "start_time": "2019-10-24T23:24:05.933521Z"
    }
   },
   "outputs": [],
   "source": [
    "def location_to_station(x, d):\n",
    "    try:\n",
    "        return d[x]\n",
    "    except KeyError as e:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:24:06.028314Z",
     "start_time": "2019-10-24T23:24:05.942791Z"
    }
   },
   "outputs": [],
   "source": [
    "df['station'] = df.location.apply(location_to_station, args=[location_station])\n",
    "df = df.dropna(subset=['station'])\n",
    "df['station'] = df['station'].astype(np.int16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:24:06.035695Z",
     "start_time": "2019-10-24T23:24:06.030879Z"
    }
   },
   "outputs": [],
   "source": [
    "big_data = True\n",
    "\n",
    "table_name = \"traffic_and_weather_190513\"\n",
    "\n",
    "cols = \"timestamp_, \\\n",
    "station, \\\n",
    "occupancy, \\\n",
    "speed, \\\n",
    "hourlyprecipitation, \\\n",
    "hourlyvisibility,\\\n",
    "hourlywindspeed\"\n",
    "\n",
    "condition = \"WHERE timestamp_ >= '2019-01-01 00:00' \\\n",
    "AND timestamp_ <  '2019-01-04 00:00'\"\n",
    "\n",
    "\n",
    "if big_data:\n",
    "    date_list = ['2019-01-01', '2019-01-15', '2019-02-10','2019-03-08']\n",
    "    q = queries(table_name, cols, date_list)\n",
    "else:\n",
    "    q = \"select \" + cols + \" from \" + table_name + \" \" + condition\n",
    "\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:24:29.189691Z",
     "start_time": "2019-10-24T23:24:06.037752Z"
    }
   },
   "outputs": [],
   "source": [
    "if big_data:\n",
    "    df_traffic_weather = read_large_table(OmnisciHandle, q)\n",
    "else:\n",
    "    df_traffic_weather = OmnisciHandle.con.select_ipc(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:24:30.225817Z",
     "start_time": "2019-10-24T23:24:29.193109Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Dataframe shape: \",df_traffic_weather.shape)\n",
    "print(\"summary of nan's\")\n",
    "print(df_traffic_weather.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:24:35.528936Z",
     "start_time": "2019-10-24T23:24:30.229081Z"
    }
   },
   "outputs": [],
   "source": [
    "# sort the dataframes to prepare for merging\n",
    "df = df.sort_values(by='timestamp_')\n",
    "df_traffic_weather = df_traffic_weather.sort_values(by='timestamp_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:24:58.322335Z",
     "start_time": "2019-10-24T23:24:35.531824Z"
    }
   },
   "outputs": [],
   "source": [
    "# add an hour_of_day column\n",
    "df_traffic_weather['hour_of_day'] = df_traffic_weather.timestamp_.dt.hour\n",
    "\n",
    "# add a day_of_week column:\n",
    "df_traffic_weather['day_of_week'] = df_traffic_weather.timestamp_.dt.dayofweek\n",
    "\n",
    "speed_grouped = df_traffic_weather.groupby(['station','day_of_week','hour_of_day'])['speed'].mean()\n",
    "occupancy_grouped = df_traffic_weather.groupby(['station','day_of_week','hour_of_day'])['occupancy'].mean()\n",
    "\n",
    "df_traffic_weather = df_traffic_weather.drop(['hour_of_day','day_of_week'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:13.701801Z",
     "start_time": "2019-10-24T23:24:58.325981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shift the speed, occupancy, so that model takes in speed and occupancy from 30 minutes previous to accident\n",
    "# TODO: TRY OUT AUTOMATED FEATURE CREATION\n",
    "# 30 minutes is 6 time steps\n",
    "df_traffic_weather[['speed_rolling30min_avg','occupancy_rolling30min_avg']] = df_traffic_weather[['speed','occupancy']].rolling(6).mean()\n",
    "\n",
    "df_traffic_weather[['speed_rolling30min_std','occupancy_rolling30min_std']] = df_traffic_weather[['speed','occupancy']].rolling(6).std()\n",
    "\n",
    "df_traffic_weather[['speed_shift10min','occupancy_shift10min']] = df_traffic_weather[['speed','occupancy']].shift(2, axis = 0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:36.181691Z",
     "start_time": "2019-10-24T23:25:13.705488Z"
    }
   },
   "outputs": [],
   "source": [
    "join_key = ['timestamp_']\n",
    "df_itw = pd.merge_asof(left=df,\n",
    "                      right=df_traffic_weather,\n",
    "                      on=join_key,\n",
    "                      by=['station'],\n",
    "                      direction='backward')\n",
    "\n",
    "print(df_itw.shape)\n",
    "df_itw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:36.188685Z",
     "start_time": "2019-10-24T23:25:36.183969Z"
    }
   },
   "outputs": [],
   "source": [
    "df_itw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:36.285214Z",
     "start_time": "2019-10-24T23:25:36.191212Z"
    }
   },
   "outputs": [],
   "source": [
    "df_itw.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:36.314331Z",
     "start_time": "2019-10-24T23:25:36.287503Z"
    }
   },
   "outputs": [],
   "source": [
    "df_itw = (df_itw.drop('postmile_state',axis=1)\n",
    "          .dropna(subset=['area'])\n",
    "         )\n",
    "\n",
    "df_itw['occupancy'] = df_itw['occupancy'].fillna(df_itw['occupancy'].mean())\n",
    "df_itw['occupancy_shift10min'] = df_itw['occupancy'].fillna(df_itw['occupancy'].mean())\n",
    "df_itw['occupancy_rolling30min_avg'] = df_itw['occupancy'].fillna(df_itw['occupancy'].mean())\n",
    "df_itw['occupancy_rolling30min_std'] = df_itw['occupancy'].fillna(df_itw['occupancy'].mean())\n",
    "\n",
    "df_itw['speed'] = df_itw['speed'].fillna(df_itw['speed'].mean())\n",
    "df_itw['speed_rolling30min_avg'] = df_itw['speed'].fillna(df_itw['speed'].mean())\n",
    "df_itw['speed_rolling30min_std'] = df_itw['speed'].fillna(df_itw['speed'].mean())\n",
    "df_itw['speed_shift10min'] = df_itw['speed'].fillna(df_itw['speed'].mean())\n",
    "\n",
    "df_itw['hourlyprecipitation'] = df_itw['hourlyprecipitation'].fillna(df_itw['hourlyprecipitation'].mean())\n",
    "df_itw['hourlyvisibility'] = df_itw['hourlyvisibility'].fillna(df_itw['hourlyvisibility'].mean())\n",
    "df_itw['hourlywindspeed'] = df_itw['hourlywindspeed'].fillna(df_itw['hourlywindspeed'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:36.330703Z",
     "start_time": "2019-10-24T23:25:36.316759Z"
    }
   },
   "outputs": [],
   "source": [
    "df_itw.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:36.337828Z",
     "start_time": "2019-10-24T23:25:36.332864Z"
    }
   },
   "outputs": [],
   "source": [
    "def diff_col(df,grouped,col):\n",
    "    l = []\n",
    "    count=1\n",
    "    for i, a in df.iterrows():\n",
    "        try:\n",
    "            l.append(a[col] - grouped[a['station'],a['day_of_week'],a['hour_of_day']] )\n",
    "        except KeyError:\n",
    "            l.append(a[col] - df[col].loc[(df['day_of_week']==a['day_of_week']) & (df['hour_of_day']==a['hour_of_day']) ].mean())\n",
    "            count=count+1\n",
    "    return l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:52.446626Z",
     "start_time": "2019-10-24T23:25:36.343475Z"
    }
   },
   "outputs": [],
   "source": [
    "speed_diff = diff_col(df_itw, speed_grouped, 'speed_shift10min')\n",
    "occupancy_diff = diff_col(df_itw, occupancy_grouped,'occupancy_shift10min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:52.461877Z",
     "start_time": "2019-10-24T23:25:52.450100Z"
    }
   },
   "outputs": [],
   "source": [
    "df_itw['speed_diff_mean'] = speed_diff\n",
    "df_itw['occupancy_diff_mean'] = occupancy_diff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:52.496401Z",
     "start_time": "2019-10-24T23:25:52.464094Z"
    }
   },
   "outputs": [],
   "source": [
    "list(df_itw.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science part\n",
    "\n",
    "Now the stuff gets interesting!\n",
    "\n",
    "let's see if we can predict the severity of an incident?\n",
    "\n",
    "also, which features are most important?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:52.522146Z",
     "start_time": "2019-10-24T23:25:52.498680Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_cols = ['incident_description','timestamp_', 'description','latitude','longitude','postmile_abs','duration','chp_code','station','speed','occupancy']\n",
    "\n",
    "data = (df_itw.drop(drop_cols, axis=1)\n",
    "        .reset_index(drop=True)\n",
    "        .copy()\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:52.533027Z",
     "start_time": "2019-10-24T23:25:52.524273Z"
    }
   },
   "outputs": [],
   "source": [
    "#Number of unique fields in category column\n",
    "category_cols = ['direction','area','freeway','hour_of_day','day_of_week']\n",
    "print(\"Cardinality of categories:\")\n",
    "for col in category_cols:\n",
    "    print(\"%s: %d\" %(col, len( data[col].unique() ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:52.551651Z",
     "start_time": "2019-10-24T23:25:52.535024Z"
    }
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:52.799922Z",
     "start_time": "2019-10-24T23:25:52.553631Z"
    }
   },
   "outputs": [],
   "source": [
    "# split data to fit in suprvised ML algorithm \n",
    "LOCATION_ENCODE='hash'\n",
    "\n",
    "# drop label column\n",
    "X_raw = data.drop('severity', axis=1)\n",
    "\n",
    "if LOCATION_ENCODE != 'ohe':\n",
    "    X_raw = X_raw.drop('location', axis=1)\n",
    "elif LOCATION_ENCODE == 'ohe':\n",
    "    category_cols.append('location')\n",
    "\n",
    "df_X = pd.get_dummies(X_raw, columns = category_cols, drop_first=True)\n",
    "\n",
    "df_X['is_ramp'] = df_X['is_ramp'].astype(int)\n",
    "df_X['is_intersection'] = df_X['is_intersection'].astype(int)\n",
    "\n",
    "if LOCATION_ENCODE is 'hash':\n",
    "    # Hash the location field\n",
    "    from sklearn.feature_extraction import FeatureHasher\n",
    "    fh = FeatureHasher(n_features=42, input_type='string')\n",
    "    hashed_features = fh.fit_transform(data['location'])\n",
    "    hashed_features = hashed_features.toarray()\n",
    "\n",
    "    print(hashed_features.shape[0] == data.shape[0])\n",
    "\n",
    "    # Add in hashed location column:\n",
    "    df_X = pd.concat([df_X, pd.DataFrame(hashed_features)], axis=1)\n",
    "\n",
    "    \n",
    "no_encoded = len(list(df_X.columns))\n",
    "print(\"{} total features after one-hot encoding and hashing.\".format(no_encoded))\n",
    "\n",
    "print(df_X.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:52.827621Z",
     "start_time": "2019-10-24T23:25:52.802157Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use numpy to convert to arrays\n",
    "\n",
    "# convert label to numpy array\n",
    "y = np.array(data['severity'])\n",
    "\n",
    "# Saving feature names for later use\n",
    "X_cols = list(df_X.columns)\n",
    "\n",
    "# Convert to numpy array\n",
    "X = np.array(df_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:52.873545Z",
     "start_time": "2019-10-24T23:25:52.830050Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.23, random_state = 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:52.882108Z",
     "start_time": "2019-10-24T23:25:52.876762Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Training Features Shape:', X_train.shape)\n",
    "print('Training Labels Shape:', y_train.shape)\n",
    "print('Testing Features Shape:', X_test.shape)\n",
    "print('Testing Labels Shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:52.897696Z",
     "start_time": "2019-10-24T23:25:52.884130Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, tree\n",
    "import xgboost\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:52.918486Z",
     "start_time": "2019-10-24T23:25:52.899736Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: TRY OUT CATBOOSTCLASSIFIER https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html\n",
    "\n",
    "# classifiers = []\n",
    "# # doesn't work!\n",
    "# # model1 = xgboost.XGBClassifier(tree_method='hist')\n",
    "# # classifiers.append(model1)\n",
    "# model1 = svm.SVC()\n",
    "# classifiers.append(model1)\n",
    "# model2 = tree.DecisionTreeClassifier()\n",
    "# classifiers.append(model2)\n",
    "# model3 = RandomForestClassifier()\n",
    "# classifiers.append(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:25:52.932685Z",
     "start_time": "2019-10-24T23:25:52.920606Z"
    }
   },
   "outputs": [],
   "source": [
    "# for clf in classifiers:\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     y_pred= clf.predict(X_test)\n",
    "#     acc = metrics.accuracy_score(y_test, y_pred)\n",
    "#     print(\"Accuracy of %s is %s\"%(clf, acc))\n",
    "#     cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "#     print(\"Confusion Matrix of %s is %s\"%(clf, cm) )\n",
    "#     print(\"roc_auc_score:\", metrics.roc_auc_score(y_test, y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:29:15.419625Z",
     "start_time": "2019-10-24T23:25:52.934852Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# set to true if you want to do hyper parameter tuning:\n",
    "RANDOM = False\n",
    "MODEL = 'GB'\n",
    "if RANDOM:\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "    print(random_grid)\n",
    "    \n",
    "        # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    clf = RandomForestClassifier()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    clf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "\n",
    "    clf_random.fit(X_train, y_train)\n",
    "    \n",
    "    print(clf_random.best_params_)\n",
    "    \n",
    "    best_random = clf_random.best_estimator_\n",
    "\n",
    "    y_pred = best_random.predict(X_test)\n",
    "\n",
    "else:\n",
    "    \n",
    "    if MODEL == 'RF':\n",
    "        # Hyperparameters have been tuned to these values:!\n",
    "        clf=RandomForestClassifier(n_estimators=2000, #2000\n",
    "                                   min_samples_split=2,\n",
    "                                   min_samples_leaf=2,\n",
    "                                   max_features='sqrt',\n",
    "                                   max_depth=20,\n",
    "                                   bootstrap=True)\n",
    "    elif MODEL == 'GB':\n",
    "        clf = GradientBoostingClassifier(n_estimators=1500,\n",
    "                                       learning_rate=0.05,\n",
    "                                       max_depth=12,\n",
    "                                       min_samples_split=2,\n",
    "                                       min_samples_leaf=2,\n",
    "                                       max_features='sqrt'\n",
    "                                        )\n",
    "        \n",
    "    clf.fit(X_train,y_train)\n",
    "\n",
    "    y_pred=clf.predict(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:29:15.456095Z",
     "start_time": "2019-10-24T23:29:15.422896Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred) )\n",
    "\n",
    "print(\"roc_auc_score:\", metrics.roc_auc_score(y_test, y_pred) )\n",
    "\n",
    "pd.crosstab(y_test, y_pred, rownames=['Actual Result'], colnames=['Predicted Result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:29:15.497477Z",
     "start_time": "2019-10-24T23:29:15.458182Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "feature_imp = pd.Series(clf.feature_importances_,index=list(df_X.columns)).sort_values(ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:29:15.813362Z",
     "start_time": "2019-10-24T23:29:15.499652Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# Creating a bar plot\n",
    "\n",
    "sns.barplot(x=feature_imp[:20], y=feature_imp.index[:20])\n",
    "# Add labels to your graph\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:29:15.820367Z",
     "start_time": "2019-10-24T23:29:15.815562Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df.freeway.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:29:15.841197Z",
     "start_time": "2019-10-24T23:29:15.822409Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_imp.loc[any('freeway')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:29:16.012258Z",
     "start_time": "2019-10-24T23:29:15.843119Z"
    }
   },
   "outputs": [],
   "source": [
    "x_values = list(range(len(feature_imp.values)))\n",
    "cumulative_importances = np.cumsum(feature_imp.values)\n",
    "# Make a line graph\n",
    "plt.plot(x_values, cumulative_importances, 'g-')\n",
    "# Draw line at 95% of importance retained\n",
    "plt.hlines(y = 0.95, xmin=0, xmax=len(feature_imp.values), color = 'r', linestyles = 'dashed')\n",
    "# Format x ticks and labels\n",
    "# plt.xticks(x_values, , rotation = 'vertical')\n",
    "# Axis labels and title\n",
    "plt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
