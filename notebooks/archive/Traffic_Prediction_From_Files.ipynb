{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T11:20:45.764349Z",
     "start_time": "2019-05-07T11:20:45.761423Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "sys.path.append('/home/mapdadmin/abraham/caltrans-data-exploration/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T11:20:47.092507Z",
     "start_time": "2019-05-07T11:20:46.232340Z"
    }
   },
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from process_traffic_data import apply_custom_transformations\n",
    "import data_processing.process_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T11:20:48.505996Z",
     "start_time": "2019-05-07T11:20:48.503514Z"
    }
   },
   "outputs": [],
   "source": [
    "from omnisci_connector.omni_connect import OmnisciConnect\n",
    "#import ibis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T11:21:50.026501Z",
     "start_time": "2019-05-07T11:21:50.022300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read configuration file /home/mapdadmin/abraham/ini_files/config.ini\n",
      "Configuration file read.\n"
     ]
    }
   ],
   "source": [
    "config_path = '/home/mapdadmin/abraham/ini_files/config.ini'\n",
    "print(\"read configuration file %s\" %config_path)\n",
    "config = ConfigParser()\n",
    "config.read(config_path)\n",
    "print(\"Configuration file read.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T11:21:50.281645Z",
     "start_time": "2019-05-07T11:21:50.278732Z"
    }
   },
   "outputs": [],
   "source": [
    "# useful columns:\n",
    "traffic_data_columns = ['timestamp_',\n",
    "                        'station',\n",
    "                        'district',\n",
    "                        'freeway',\n",
    "                        'direction',\n",
    "                        'lane_type',\n",
    "                        'station_length',\n",
    "                        'samples',\n",
    "                        'pct_observed',\n",
    "                        'total_flow',\n",
    "                        'occupancy',\n",
    "                        'speed'\n",
    "                        ]\n",
    "\n",
    "traffic_meta_columns = ['ID',\n",
    "                        'County',\n",
    "                        'State_PM',\n",
    "                        'Abs_PM',\n",
    "                        'Latitude',\n",
    "                        'Longitude',\n",
    "                        'Lanes',\n",
    "                        'Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T11:21:50.531911Z",
     "start_time": "2019-05-07T11:21:50.515623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traffic metadata file read.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### metadata section ###\n",
    "traffic_meta_path = config.get('Paths', 'meta_path')\n",
    "\n",
    "# read in the traffic metadata to pandas:\n",
    "df_traffic_metadata = pd.read_csv(traffic_meta_path, sep='\\t', usecols=traffic_meta_columns).set_index('ID')\n",
    "df_traffic_metadata = df_traffic_metadata.rename(str.lower, axis='columns')\n",
    "print(\"traffic metadata file read.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T11:21:50.922909Z",
     "start_time": "2019-05-07T11:21:50.920450Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_year(paths, year):\n",
    "    return [f for f in paths if year in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T11:27:46.976913Z",
     "start_time": "2019-05-07T11:27:46.973107Z"
    }
   },
   "outputs": [],
   "source": [
    "# initial parameters for reading in traffic data\n",
    "threshold = 0.05\n",
    "interest_col = 'speed'\n",
    "grouper = 'station'\n",
    "batch_limit = 5 #if running in parallel, set to number of parallel threads\n",
    "file_ext = '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T11:27:50.578443Z",
     "start_time": "2019-05-07T11:27:50.573885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of traffic files:  48\n"
     ]
    }
   ],
   "source": [
    "### Traffic Section ###\n",
    "\n",
    "# get the paths of relevant files for the data\n",
    "csv_files = config.get('Paths', 'data_path')\n",
    "file_paths = utils.get_file_names(csv_files, extension=file_ext)\n",
    "\n",
    "#file_paths = get_year(file_paths, '2019')\n",
    "\n",
    "file_paths = sorted(file_paths)\n",
    "\n",
    "print(\"Number of traffic files: \",len(file_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T11:27:53.806528Z",
     "start_time": "2019-05-07T11:27:53.802542Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_time_transformations(df):\n",
    "    df['timestamp_'] = pd.to_datetime(df['timestamp_'], infer_datetime_format=True)\n",
    "\n",
    "    # add a rounded timestamp for grouping later on: (e.g. 12:46 --> 13:00)\n",
    "    df = utils.rounded_timestamp(df=df, name_current_ts='timestamp_', name_rounded_ts='timestamp_rounded', round_by='H')\n",
    "\n",
    "    df = utils.add_day_of_week(df, 'timestamp_')\n",
    "\n",
    "    # add column with hour of day for each data point\n",
    "    df['hour_of_day'] = df['timestamp_'].dt.hour\n",
    "\n",
    "    # add column with day of year for each data point\n",
    "    df['day_of_year'] = df['timestamp_'].dt.dayofyear\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T11:27:57.407498Z",
     "start_time": "2019-05-07T11:27:57.403266Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_custom_transformations(df, interest_col, threshold, grouper):\n",
    "\n",
    "    # drop nas\n",
    "    df = utils.grouped_drop_na(df, threshold, grouper=grouper, col=interest_col)\n",
    "\n",
    "    # lower all column names\n",
    "    df = utils.lower_col_names(df)\n",
    "\n",
    "    df['state_pm'] = utils.state_pm_to_numeric(df['state_pm'])\n",
    "    \n",
    "    df = apply_time_transformations(df)\n",
    "\n",
    "    # drop all rows with na in absolute postmarker field\n",
    "    # df = df.dropna(subset=['abs_pm'])\n",
    "\n",
    "    # downcast all ints to save memory\n",
    "    # df = utils.downcast_int(df, ['station', 'freeway', 'samples', 'total_flow', 'lanes', 'county'])\n",
    "\n",
    "    # downcast all floats to save memory\n",
    "    # df = utils.downcast_type(df)\n",
    "\n",
    "    df = df.drop(['station_length','name', 'pct_observed','abs_pm'],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T11:49:43.420740Z",
     "start_time": "2019-05-07T11:35:42.283174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2015_01.txt\n",
      "time to read_csv: 3.675838\n",
      "time to append: 0.000548\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2015_02.txt\n",
      "time to read_csv: 3.136148\n",
      "time to append: 0.000124\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2015_03.txt\n",
      "time to read_csv: 3.689518\n",
      "time to append: 0.000138\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2015_04.txt\n",
      "time to read_csv: 3.611367\n",
      "time to append: 0.000135\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2015_05.txt\n",
      "time to read_csv: 3.763702\n",
      "time to append: 0.000266\n",
      "time to concat: 1.901461\n",
      "time to join: 4.466563\n",
      "time to transform: 52.598141\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2015_06.txt\n",
      "time to read_csv: 3.672598\n",
      "time to append: 0.000305\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2015_07.txt\n",
      "time to read_csv: 3.741105\n",
      "time to append: 0.000422\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2015_08.txt\n",
      "time to read_csv: 3.810983\n",
      "time to append: 0.000138\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2015_09.txt\n",
      "time to read_csv: 3.636057\n",
      "time to append: 0.000435\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2015_10.txt\n",
      "time to read_csv: 3.818374\n",
      "time to append: 0.000132\n",
      "time to concat: 1.933259\n",
      "time to join: 4.587436\n",
      "time to transform: 53.563833\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2015_11.txt\n",
      "time to read_csv: 3.679026\n",
      "time to append: 0.000261\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2015_12.txt\n",
      "time to read_csv: 3.818981\n",
      "time to append: 0.000510\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2016_01.txt\n",
      "time to read_csv: 3.785792\n",
      "time to append: 0.000140\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2016_02.txt\n",
      "time to read_csv: 3.466106\n",
      "time to append: 0.000415\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2016_03.txt\n",
      "time to read_csv: 3.899900\n",
      "time to append: 0.000131\n",
      "time to concat: 2.048412\n",
      "time to join: 4.609411\n",
      "time to transform: 52.495363\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2016_04.txt\n",
      "time to read_csv: 3.641736\n",
      "time to append: 0.000142\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2016_05.txt\n",
      "time to read_csv: 3.908060\n",
      "time to append: 0.000445\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2016_06.txt\n",
      "time to read_csv: 3.912297\n",
      "time to append: 0.000807\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2016_07.txt\n",
      "time to read_csv: 4.095614\n",
      "time to append: 0.000142\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2016_08.txt\n",
      "time to read_csv: 4.147374\n",
      "time to append: 0.000136\n",
      "time to concat: 2.107084\n",
      "time to join: 4.974904\n",
      "time to transform: 51.614297\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2016_09.txt\n",
      "time to read_csv: 3.954148\n",
      "time to append: 0.000146\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2016_10.txt\n",
      "time to read_csv: 4.191998\n",
      "time to append: 0.000141\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2016_11.txt\n",
      "time to read_csv: 4.170165\n",
      "time to append: 0.000267\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2016_12.txt\n",
      "time to read_csv: 4.310771\n",
      "time to append: 0.000132\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2017_01.txt\n",
      "time to read_csv: 4.304810\n",
      "time to append: 0.000133\n",
      "time to concat: 2.316583\n",
      "time to join: 5.147351\n",
      "time to transform: 53.237032\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2017_02.txt\n",
      "time to read_csv: 3.742734\n",
      "time to append: 0.000437\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2017_03.txt\n",
      "time to read_csv: 4.192171\n",
      "time to append: 0.000267\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2017_04.txt\n",
      "time to read_csv: 4.150050\n",
      "time to append: 0.000134\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2017_05.txt\n",
      "time to read_csv: 4.421785\n",
      "time to append: 0.000160\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2017_06.txt\n",
      "time to read_csv: 4.302926\n",
      "time to append: 0.000141\n",
      "time to concat: 2.229558\n",
      "time to join: 5.184118\n",
      "time to transform: 52.842823\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2017_07.txt\n",
      "time to read_csv: 6.603761\n",
      "time to append: 0.000408\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2017_08.txt\n",
      "time to read_csv: 6.644873\n",
      "time to append: 0.000123\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2017_09.txt\n",
      "time to read_csv: 6.463681\n",
      "time to append: 0.000117\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2017_10.txt\n",
      "time to read_csv: 6.681980\n",
      "time to append: 0.000145\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2017_11.txt\n",
      "time to read_csv: 6.437375\n",
      "time to append: 0.000446\n",
      "time to concat: 2.253185\n",
      "time to join: 5.344063\n",
      "time to transform: 53.279866\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2017_12.txt\n",
      "time to read_csv: 6.648833\n",
      "time to append: 0.000140\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2018_01.txt\n",
      "time to read_csv: 6.590426\n",
      "time to append: 0.000133\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2018_02.txt\n",
      "time to read_csv: 6.104538\n",
      "time to append: 0.000132\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2018_03.txt\n",
      "time to read_csv: 6.758024\n",
      "time to append: 0.000268\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2018_04.txt\n",
      "time to read_csv: 6.583181\n",
      "time to append: 0.000453\n",
      "time to concat: 2.351746\n",
      "time to join: 5.196320\n",
      "time to transform: 52.372347\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2018_05.txt\n",
      "time to read_csv: 6.614427\n",
      "time to append: 0.000146\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2018_06.txt\n",
      "time to read_csv: 6.434815\n",
      "time to append: 0.000128\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2018_07.txt\n",
      "time to read_csv: 6.747303\n",
      "time to append: 0.000802\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2018_08.txt\n",
      "time to read_csv: 6.842480\n",
      "time to append: 0.000141\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2018_09.txt\n",
      "time to read_csv: 6.439790\n",
      "time to append: 0.000136\n",
      "time to concat: 2.344136\n",
      "time to join: 5.208316\n",
      "time to transform: 54.318383\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2018_10.txt\n",
      "time to read_csv: 6.572407\n",
      "time to append: 0.000156\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2018_11.txt\n",
      "time to read_csv: 6.483301\n",
      "time to append: 0.000262\n",
      "Processing file:  /home/mapdadmin/abraham/data_caltrans/hour/d04_text_station_hour_2018_12.txt\n",
      "time to read_csv: 6.692841\n",
      "time to append: 0.000395\n",
      "time to concat: 1.525939\n",
      "time to join: 3.120771\n",
      "time to transform: 31.565406\n"
     ]
    }
   ],
   "source": [
    "df_out = []\n",
    "# extract and traffic data in batches:\n",
    "no_data_cols = list(range(len(traffic_data_columns)))\n",
    "for i in range(0, len(file_paths), batch_limit):\n",
    "#for i in range(0, batch_limit, batch_limit):\n",
    "    df_batch = []\n",
    "    for f in file_paths[i:i + batch_limit]:\n",
    "        print(\"Processing file: \", f)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        temp = pd.read_csv(f, header=None, names=traffic_data_columns, usecols=no_data_cols)\n",
    "        t1 = time.time()\n",
    "        print(\"time to read_csv: %f\" %(t1-t0))\n",
    "        df_batch.append(temp)\n",
    "        t2 = time.time()\n",
    "        print(\"time to append: %f\" %(t2-t1))\n",
    "        \n",
    "    df_extracted_traffic = pd.concat(df_batch, ignore_index=True)\n",
    "    t3 = time.time()\n",
    "    print(\"time to concat: %f\" %(t3-t2))\n",
    "    \n",
    "    df_extracted_traffic = df_extracted_traffic.drop('district', axis=1)\n",
    "    \n",
    "    t4 = time.time()\n",
    "    df_extracted_traffic = df_extracted_traffic.join(df_traffic_metadata, on='station')\n",
    "    t5 = time.time()\n",
    "    print(\"time to join: %f\" %(t5-t4))\n",
    "    try:\n",
    "        df_transformed_traffic = apply_custom_transformations(df=df_extracted_traffic,\n",
    "                                                              interest_col=interest_col,\n",
    "                                                              threshold=threshold,\n",
    "                                                              grouper=grouper)\n",
    "        t6 = time.time()\n",
    "        print(\"time to transform: %f\" %(t6-t5))\n",
    "        \n",
    "        df_out.append(df_transformed_traffic) \n",
    "    except ValueError as ex:\n",
    "        print(ex)\n",
    "        print(\"Skipping Batch starting at: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T11:50:34.557016Z",
     "start_time": "2019-05-07T11:50:14.991228Z"
    }
   },
   "outputs": [],
   "source": [
    "df_full = pd.concat(df_out, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T12:28:50.138176Z",
     "start_time": "2019-05-07T12:28:50.118985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_</th>\n",
       "      <th>station</th>\n",
       "      <th>freeway</th>\n",
       "      <th>direction</th>\n",
       "      <th>lane_type</th>\n",
       "      <th>samples</th>\n",
       "      <th>total_flow</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>speed</th>\n",
       "      <th>county</th>\n",
       "      <th>state_pm</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>lanes</th>\n",
       "      <th>timestamp_rounded</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_week_num</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>day_of_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83695143</th>\n",
       "      <td>2018-12-31 23:00:00</td>\n",
       "      <td>422116</td>\n",
       "      <td>101</td>\n",
       "      <td>N</td>\n",
       "      <td>ML</td>\n",
       "      <td>600</td>\n",
       "      <td>2747.0</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>71.9</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>37.456467</td>\n",
       "      <td>-122.133857</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018-12-31 23:00:00</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83695144</th>\n",
       "      <td>2018-12-31 23:00:00</td>\n",
       "      <td>422161</td>\n",
       "      <td>101</td>\n",
       "      <td>S</td>\n",
       "      <td>ML</td>\n",
       "      <td>599</td>\n",
       "      <td>2439.0</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>70.9</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>37.456130</td>\n",
       "      <td>-122.133712</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018-12-31 23:00:00</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83695145</th>\n",
       "      <td>2018-12-31 23:00:00</td>\n",
       "      <td>422357</td>\n",
       "      <td>880</td>\n",
       "      <td>N</td>\n",
       "      <td>ML</td>\n",
       "      <td>360</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>67.6</td>\n",
       "      <td>85.0</td>\n",
       "      <td>3.61</td>\n",
       "      <td>37.358660</td>\n",
       "      <td>-121.906756</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2018-12-31 23:00:00</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83695146</th>\n",
       "      <td>2018-12-31 23:00:00</td>\n",
       "      <td>422359</td>\n",
       "      <td>880</td>\n",
       "      <td>N</td>\n",
       "      <td>ML</td>\n",
       "      <td>479</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>69.3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>5.27</td>\n",
       "      <td>37.380936</td>\n",
       "      <td>-121.904008</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018-12-31 23:00:00</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83695147</th>\n",
       "      <td>2018-12-31 23:00:00</td>\n",
       "      <td>422749</td>\n",
       "      <td>780</td>\n",
       "      <td>E</td>\n",
       "      <td>ML</td>\n",
       "      <td>240</td>\n",
       "      <td>387.0</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>65.9</td>\n",
       "      <td>95.0</td>\n",
       "      <td>7.27</td>\n",
       "      <td>38.091824</td>\n",
       "      <td>-122.231958</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-12-31 23:00:00</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp_  station  freeway direction lane_type  samples  \\\n",
       "83695143 2018-12-31 23:00:00   422116      101         N        ML      600   \n",
       "83695144 2018-12-31 23:00:00   422161      101         S        ML      599   \n",
       "83695145 2018-12-31 23:00:00   422357      880         N        ML      360   \n",
       "83695146 2018-12-31 23:00:00   422359      880         N        ML      479   \n",
       "83695147 2018-12-31 23:00:00   422749      780         E        ML      240   \n",
       "\n",
       "          total_flow  occupancy  speed  county  state_pm   latitude  \\\n",
       "83695143      2747.0     0.0242   71.9    81.0      0.42  37.456467   \n",
       "83695144      2439.0     0.0286   70.9    81.0      0.40  37.456130   \n",
       "83695145      1124.0     0.0229   67.6    85.0      3.61  37.358660   \n",
       "83695146      1301.0     0.0206   69.3    85.0      5.27  37.380936   \n",
       "83695147       387.0     0.0162   65.9    95.0      7.27  38.091824   \n",
       "\n",
       "           longitude  lanes   timestamp_rounded day_of_week  day_of_week_num  \\\n",
       "83695143 -122.133857    5.0 2018-12-31 23:00:00      Monday                0   \n",
       "83695144 -122.133712    5.0 2018-12-31 23:00:00      Monday                0   \n",
       "83695145 -121.906756    3.0 2018-12-31 23:00:00      Monday                0   \n",
       "83695146 -121.904008    4.0 2018-12-31 23:00:00      Monday                0   \n",
       "83695147 -122.231958    2.0 2018-12-31 23:00:00      Monday                0   \n",
       "\n",
       "          hour_of_day  day_of_year  \n",
       "83695143           23          365  \n",
       "83695144           23          365  \n",
       "83695145           23          365  \n",
       "83695146           23          365  \n",
       "83695147           23          365  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T11:57:07.042415Z",
     "start_time": "2019-05-07T11:56:07.242052Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump( df_full, open( \"df_hour2.p\", \"wb\" ),protocol=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:18:37.519539Z",
     "start_time": "2019-05-07T10:18:30.875750Z"
    }
   },
   "outputs": [],
   "source": [
    "load = True\n",
    "import pickle\n",
    "if load:\n",
    "    df = pickle.load(open( \"df_testing.p\", \"rb\" ) )\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:18:44.115807Z",
     "start_time": "2019-05-07T10:18:38.341757Z"
    }
   },
   "outputs": [],
   "source": [
    "# freeway 101\n",
    "len(df.loc[df['freeway']==101].station.unique())\n",
    "\n",
    "#Select highway 101\n",
    "df_101 = df.loc[df['freeway']==101]\n",
    "\n",
    "#select north direction\n",
    "df_101N = df_101.loc[df_101['direction']=='N']\n",
    "\n",
    "#stations ordered by mile marker\n",
    "stations = df_101N[['station','state_pm']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:18:44.599492Z",
     "start_time": "2019-05-07T10:18:44.575699Z"
    }
   },
   "outputs": [],
   "source": [
    "stations = df_101N[['station','state_pm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:18:47.437924Z",
     "start_time": "2019-05-07T10:18:45.054510Z"
    }
   },
   "outputs": [],
   "source": [
    "df_101.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: simple time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:18:47.904552Z",
     "start_time": "2019-05-07T10:18:47.892472Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 1: simple \n",
    "station_1 = df_101N.loc[df_101N['station']==400001]\n",
    "\n",
    "cols = ['timestamp_','total_flow','occupancy','speed']\n",
    "station_1 = station_1[cols]\n",
    "\n",
    "station_1 = station_1.set_index('timestamp_')\n",
    "cols = station_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:18:50.273661Z",
     "start_time": "2019-05-07T10:18:49.471959Z"
    }
   },
   "outputs": [],
   "source": [
    "station_1.plot(subplots=True, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:41:43.277541Z",
     "start_time": "2019-05-07T09:41:43.271796Z"
    }
   },
   "outputs": [],
   "source": [
    "def tsplot(y, title, lags=None, figsize=(10, 6)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    layout = (2, 2)\n",
    "    ts_ax = plt.subplot2grid(layout, (0, 0))\n",
    "    hist_ax = plt.subplot2grid(layout, (0, 1))\n",
    "    acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "    pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "\n",
    "    y.plot(ax=ts_ax)\n",
    "    ts_ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    y.plot(ax=hist_ax, kind='hist', bins=25)\n",
    "    hist_ax.set_title('Histogram')\n",
    "    sm.tsa.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "    sm.tsa.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ts_ax, acf_ax, pacf_ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:42:04.865063Z",
     "start_time": "2019-05-07T09:41:47.514539Z"
    }
   },
   "outputs": [],
   "source": [
    "tsplot(station_1['speed'],'speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:42:13.217155Z",
     "start_time": "2019-05-07T09:42:13.211440Z"
    }
   },
   "outputs": [],
   "source": [
    "station_1 = station_1.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:00:10.069312Z",
     "start_time": "2019-05-07T09:59:40.878773Z"
    }
   },
   "outputs": [],
   "source": [
    "model = sm.tsa.VARMAX(train, order=(2, 0), trend='c')\n",
    "model_result = model.fit(maxiter=1000, disp=False)\n",
    "model_result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:42:26.024017Z",
     "start_time": "2019-05-07T09:42:25.999860Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating the train and validation set\n",
    "train = station_1[:int(0.8*(len(station_1)))]\n",
    "valid = station_1[int(0.8*(len(station_1))):]\n",
    "\n",
    "#fit the model\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "model = VAR(endog=train)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# make prediction on validation\n",
    "prediction = model_fit.forecast(model_fit.y, steps=len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:42:36.035403Z",
     "start_time": "2019-05-07T09:42:35.763743Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:42:54.417662Z",
     "start_time": "2019-05-07T09:42:53.665078Z"
    }
   },
   "outputs": [],
   "source": [
    "#converting predictions to dataframe\n",
    "from sklearn.metrics import mean_squared_error\n",
    "pred = pd.DataFrame(index=range(0,len(prediction)),columns=[cols])\n",
    "for j in range(0,len(cols)):\n",
    "    for i in range(0, len(prediction)):\n",
    "        pred.iloc[i][j] = prediction[i][j]\n",
    "\n",
    "#check rmse\n",
    "for i in cols:\n",
    "    print('rmse value for', i, 'is : ', np.sqrt(mean_squared_error(pred[i], valid[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:47:16.265348Z",
     "start_time": "2019-05-07T09:47:16.261558Z"
    }
   },
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:47:22.329390Z",
     "start_time": "2019-05-07T09:47:22.325993Z"
    }
   },
   "outputs": [],
   "source": [
    "valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:03:04.823785Z",
     "start_time": "2019-05-07T10:03:04.628930Z"
    }
   },
   "outputs": [],
   "source": [
    "pred['speed'].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:50:02.419458Z",
     "start_time": "2019-05-07T09:50:02.216153Z"
    }
   },
   "outputs": [],
   "source": [
    "valid['speed'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:02:59.750455Z",
     "start_time": "2019-05-07T10:02:59.729084Z"
    }
   },
   "outputs": [],
   "source": [
    "#make final predictions\n",
    "model = VAR(endog=station_1)\n",
    "model_fit = model.fit()\n",
    "yhat = model_fit.forecast(model_fit.y, steps=5)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:19:23.055616Z",
     "start_time": "2019-05-07T10:19:23.048713Z"
    }
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:19:24.892921Z",
     "start_time": "2019-05-07T10:19:24.753514Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:39:16.282247Z",
     "start_time": "2019-05-07T10:39:16.262768Z"
    }
   },
   "outputs": [],
   "source": [
    "# process data\n",
    "values = station_1.values\n",
    "\n",
    "\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler1 = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "\n",
    "scaled1 = scaler1.fit_transform(values[:,2].reshape(-1, 1))\n",
    "\n",
    "\n",
    "reframed = series_to_supervised(scaled,int(60/5))\n",
    "reframed.drop(['var2(t)','var1(t)'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:39:23.742839Z",
     "start_time": "2019-05-07T10:39:23.719430Z"
    }
   },
   "outputs": [],
   "source": [
    "reframed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:39:24.763000Z",
     "start_time": "2019-05-07T10:39:24.760421Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:40:45.308523Z",
     "start_time": "2019-05-07T10:40:45.301939Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "train = values[:int(0.8*(len(station_1))), :]\n",
    "test = values[int(0.8*(len(station_1))):, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:41:05.322899Z",
     "start_time": "2019-05-07T10:40:46.012161Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:41:06.160104Z",
     "start_time": "2019-05-07T10:41:05.983139Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:41:52.871588Z",
     "start_time": "2019-05-07T10:41:52.566543Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:42:14.140919Z",
     "start_time": "2019-05-07T10:42:14.138083Z"
    }
   },
   "outputs": [],
   "source": [
    "# invert scaling for forecast\n",
    "inv_yhat = np.concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler1.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:42:25.836550Z",
     "start_time": "2019-05-07T10:42:25.832278Z"
    }
   },
   "outputs": [],
   "source": [
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = np.concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler1.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:42:39.228241Z",
     "start_time": "2019-05-07T10:42:39.222237Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# calculate RMSE\n",
    "rmse = math.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:43:30.524716Z",
     "start_time": "2019-05-07T10:43:30.516411Z"
    }
   },
   "outputs": [],
   "source": [
    "inv_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T10:43:33.466462Z",
     "start_time": "2019-05-07T10:43:33.279044Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(inv_y)\n",
    "plt.plot(inv_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:traffic_demo]",
   "language": "python",
   "name": "conda-env-traffic_demo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
